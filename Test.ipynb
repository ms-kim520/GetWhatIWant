{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Giveme5W1H 코드 해체\n",
    "출처 : https://github.com/fhamborg/Giveme5W1H\n",
    "- 목적 :\n",
    "    * PROMED 기사에서 육하원칙을 룰 기반으로 추출해내는 것\n",
    "    *  Giveme5W1H의 원리를 사용(후보군 추출 후 점수)\n",
    "- 그 중에서도 cause extractor를 사용하여서 질병의 원인, 감염원, 증상 등을 추출하는 것을 목적으로 한다\n",
    "- 나머지 육하원칙은 그리 중요하지는 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#예문\n",
    "text = ' A vegetable farm in Chongpyong County [South Hamgyong province] suffered from poor crop yields due to pests and disease. The farm produces cabbage and cucumber, but the cabbage began rotting from the roots and falling apart due to damage from an unidentified disease, along with pests. The farm was only able to meet 40% of its production target.The farmers attempted to salvage the situation on their own, but were ultimately unable to do so. [They] requested that authorities find ways to resolve the problem, noting that even if the rotting cabbage was quickly harvested and delivered to Pyongyang, it would likely be completely rotten by the time it arrived.The authorities quickly formed a team of \"plant disease control experts\" [which] discovered that instead of purple cabbage seeds, the farm had planted regular large cabbage seeds. This led to disease and pests appearing on the roots. The farm already knew that only the purple seeds were suitable for the soil, [but] were unable to acquire any of the right seeds last year [2019].Farmers are reportedly in the process of fumigating their facilities and turning the rotten cabbage into compost.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 기존 Giveme5W1H의 사용법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Standford-NLP Server를 구동해야 함\n",
    "# stanford-corenlp-full-2017-06-09.jar 파일 다운로드 후, runtime-resources 폴더에 압축 풀기\n",
    "# cd C:\\Users\\ms.kim1\\AppData\\Local\\Programs\\Python\\Python36\\Lib\\site-packages\\Giveme5W1H\\runtime-resources\\stanford-corenlp-full-2017-06-09\n",
    "# java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "how : from poor crop yields due to pests and disease .\n"
    }
   ],
   "source": [
    "from Giveme5W1H.extractor.document import Document\n",
    "from Giveme5W1H.extractor.extractor import MasterExtractor\n",
    "\n",
    "extractor = MasterExtractor() #extractor object 만들기\n",
    "doc = Document.from_text(text) #document object 만들기\n",
    "# or: doc = Document(title, lead, text, date_publish) \n",
    "doc = extractor.parse(doc) #parse의 기능 : 전처리 -> 육하원칙 후보 구문 추출 -> 점수 매기기 -> 각 분야에서 가장 높은 점수를 얻은 구문을 답으로 반환\n",
    "\n",
    "top_who_answer = doc.get_top_answer('why').get_parts_as_text() #why : the farm\n",
    "top_who_answer = doc.get_top_answer('how').get_parts_as_text() #how : from poor crop yields due to pests and disease .\n",
    "print('how : '+top_who_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나, 그리 만족스러운 답을 얻지는 못함. 코드를 해체해서 커스터마이징을 하기로 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PreProcess 과정 이용하기\n",
    "- 먼저, how, why 등과 같은 요소를 얻고 싶기 때문에 cause_extractor를 커스터마이징하기로 결정\n",
    "\n",
    "##### 만났던 문제점\n",
    "- cause_extractor 상에서 document.get_trees()란 메소드를 발견, document.py 상에서는 그냥 하나의 오브젝트를 초기화하거나 call하는 정도의 기능만 있음.\n",
    "- 알고보니, tree란 구조는 stardford corenlp server를 통해 나온 결과의 포맷이었음.\n",
    "- 즉, cause_extractor object를 사용하려면 tree구조의 document를 얻어야 했음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#필요 패키지 선언\n",
    "from Giveme5W1H.extractor.document import Document\n",
    "from Giveme5W1H.extractor.preprocessors.preprocessor_core_nlp import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document.from_text(text) #document 객체 생성\n",
    "pre_text=Preprocessor() #Preprocessor 객체 만들기\n",
    "doc2=pre_text.preprocess(doc) #전처리 객체에서 document객체를 이용해서 전처리 method 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "NoneType"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "type(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "왜인지, 제대로 적용이 되지 않음. 이는 아직 코드의 구조에 대해 제대로 이해하지 못해서 method가 정확하게 call이 되지 않은 것으로 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 직접 standford server nlp를 사용하여 전처리를 수행하여 tree구조를 구하기로 결정\n",
    "- 뒷 과정은 함수를 그냥 하나하나 다 뜯어서 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Standford-NLP Server를 구동해야 함\n",
    "# stanford-corenlp-full-2017-06-09.jar 파일 다운로드 후, runtime-resources 폴더에 압축 풀기\n",
    "# cd C:\\Users\\ms.kim1\\AppData\\Local\\Programs\\Python\\Python36\\Lib\\site-packages\\Giveme5W1H\\runtime-resources\\stanford-corenlp-full-2017-06-09\n",
    "# java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. 필요한 패키지 선언\n",
    "import nltk\n",
    "from pycorenlp import StanfordCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. annotation획득\n",
    "nlp=StanfordCoreNLP('http://localhost:9000')\n",
    "actual_config=base_config = {\n",
    "            'timeout': 500000,\n",
    "            'annotators': 'tokenize,ssplit,pos,lemma,parse,ner,depparse,mention,coref',\n",
    "            'tokenize.language': 'English',\n",
    "            'outputFormat': 'json'\n",
    "        }\n",
    "res=nlp.annotate(text,actual_config)\n",
    "#Giveme5W1H에서 선언한 configuration요소들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res['sentences'][0] \n",
    "# 각 문장별로, index, parse, position, coref 등의 요소 반환해줌(json 형식으로, dictionary 타입임)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. tree구조를 만들기로 함\n",
    "annotation = res\n",
    "tree=[] \n",
    "for sentence in annotation['sentences']:\n",
    "    _token_index = 0\n",
    "    _tokens = sentence['tokens']\n",
    "    sentence_tree=nltk.Tree.fromstring(sentence['parse'])\n",
    "    # sentence_tree = nltk.ParentedTree.fromstring(sentence['parse']) #read_leaf 함수가 없어서 삭제한 버전으로, 나중에 evaluate tree함수 부분에서 pos가 반환이 안됨\n",
    "    # sentence_tree = nltk.ParentedTree.fromstring(sentence['parse'],read_leaf=_link_leaf_to_core_nlp()) #pos가 나오게 하는 read_leaf 사용하기\n",
    "    # add a reference to the original data from parsing for this sentence\n",
    "    sentence_tree.stanfordCoreNLPResult = sentence\n",
    "\n",
    "    tree.append(sentence_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 때, token요소만 따로 추출해서, 따로 선언했던 _link_leaf_to_core_nlp에 인자로 던져주는 로직이 있었고, 그것의 결과를 tree 구조를 구할 때 read_leaf의 인자로 반환하는 코드가 있었다. 확인해본 결과, read_leaf는 반환되는 tree구조의 쓰이는 양식으로 보임. 또, _link_leaf_to_core_nlp는 문장의 token관련 요소의 값에 따라 반환되는 그 형식과 결과가 달라졌다. token의 길이가 1보다 작은 경우와 큰 경우로 나누어져 있었다. read_leaf에 result를 따로 저장하여 입력했지만, 계속 error가 나오는 관계로 일단 제쳐두고 tree구조의 결과값을 받았다. \n",
    "(Giveme5W1H > extractor > preprocessors > preprocessor_core_nlp.py > _link_leaf_to_core_nlp함수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "coerfs = annotation['corefs']\n",
    "#문장 속 단어간의 상관계수를 저장하는 변수로 보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [] #단어의 토큰화 : 문장 속 단어를 쪼개는 작업\n",
    "pos = []  # 문장 속 단어의 위치\n",
    "ner = [] # 개체명인식 : 문맥상 의미를 파악하여 entity를 추출하여 미리 정의된 분류체계로 정보를 분류하는 것\n",
    "\n",
    "for sentence in annotation['sentences']:\n",
    "    s_tokens = []\n",
    "    s_pos = []\n",
    "    s_ner = []\n",
    "    for token in sentence['tokens']:\n",
    "        s_tokens.append(token)\n",
    "        s_pos.append((token['originalText'], token['pos'])) #원문 + 위치\n",
    "        s_ner.append((token['originalText'], token['ner'])) #원문 + ner\n",
    "\n",
    "    tokens.append(s_tokens)\n",
    "    pos.append(s_pos)\n",
    "    ner.append(s_ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tree구조 이외로도 필요한 요소들을 따로 저장해두는 작업을 진행.(coerfs, tokens, position, NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PreProcessing 과정 후 cause_extractor 부분을 실행하면서 그 결과가 어떻게 나온 것인지 확인하기로 함\n",
    "- 본격적으로 tree구조를 사용하여 바로 cause를 뽑아내기 전, 개발자가 원인을 뽑아내기 위해 구축한 간단한 사전들을 온톨로지를 활용하여 쪼개놓는 작업을 먼저 해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#1. Frinston에서 제작한 영어 단어 온톨로지 wordnet을 활용/필요 패키지 다운 및 선언\n",
    "# import nltk\n",
    "# nltk.download('wordnet') \n",
    "\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. 개발자가 선언한 casual_verbs : 원인과 보통 함께 쓰이는 동사를 모음\n",
    "causal_verbs = ['activate', 'actuate', 'arouse', 'associate', 'begin', 'bring', 'call', 'cause', 'commence',\n",
    "                    'conduce', 'contribute', 'create', 'derive', 'develop', 'educe', 'effect', 'effectuate', 'elicit',\n",
    "                    'entail', 'evoke', 'fire', 'generate', 'give', 'implicate', 'induce', 'kick', 'kindle', 'launch',\n",
    "                    'lead', 'link', 'make', 'originate', 'produce', 'provoke', 'put', 'relate', 'result', 'rise', 'set',\n",
    "                    'spark', 'start', 'stem', 'stimulate', 'stir', 'trigger', 'unleash']\n",
    "\n",
    "# translate causal verbs into synsets(유의어 추출)\n",
    "synsets = []\n",
    "for verb in causal_verbs:\n",
    "    synsets += wordnet.synsets(verb, 'v')\n",
    "    causal_verbs = set(synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verbs involved in NP-VP-NP constraints(문법 형식에 따라 다르게 나타나는 동사군)\n",
    "constraints_verbs = {'cause': None, 'associate': None, 'relate': None, 'lead': None, 'induce': None}\n",
    "\n",
    "for verb in constraints_verbs:\n",
    "    constraints_verbs[verb] = set(wordnet.synsets(verb, 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyponym:하위어/\n",
    "def get_hyponyms(synsets):\n",
    "    \"\"\"\n",
    "    Fetches all hyponyms in a recursive manner creating a word class.\n",
    "\n",
    "    :param synsets: The list of synsets to process\n",
    "    :type synsets: [synset]\n",
    "\n",
    "    :return: A set of synsets\n",
    "    \"\"\"\n",
    "\n",
    "    result = set()\n",
    "    for hyponym in synsets.hyponyms():\n",
    "        result |= get_hyponyms(hyponym) #|= : 비트or 연산 후 할당/0 0 -> 0, 0 1 -> 1, 1 1 -> 1\n",
    "    return result | set(synsets.hyponyms())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyponym classes involved in NP-VP-NP constraints\n",
    "# hyponym : ~~에 속하는, 목, 속 \n",
    "constraints_hyponyms = {'entity': None, 'phenomenon': None, 'abstraction': None, 'group': None, 'possession': None,\n",
    "                        'event': None, 'act': None, 'state': None}\n",
    "# initialize synsets that are used as constraints in NP-VP-VP patterns\n",
    "for noun in constraints_hyponyms:\n",
    "    hyponyms = set()\n",
    "    for synset in wordnet.synsets(noun, 'n'):\n",
    "        hyponyms |= get_hyponyms(synset)\n",
    "    constraints_hyponyms[noun] = hyponyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#형태소분석기 : 공통기저형인 어휘소/어간\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = (.56,.44,.27,.026) #evaluate하기 위한 가중치 set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원인에 해당할만한 구문을 추출하기 위한 코드. 이 때 tree 구조를 평가한 결과를 기반으로 구문 후보를 뽑아내기 때문에, tree 평가 함수를 사용하도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-05a4da684212>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpostrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mcandidate\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_evaluate_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mcandidateObject\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCandidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# used by the extractor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract Candidate\n",
    "\"\"\"\n",
    "Extracts possible agents/actions pairs from a given document.\n",
    "Candidates are chosen if they belong to an coref-chain and is part of a NP-VP-NP pattern\n",
    "\n",
    ":param document: The Document to be analyzed.\n",
    ":type document: Document\n",
    "\n",
    ":return: A List of Tuples containing all agents, actions and their position in the document.\n",
    "\"\"\"\n",
    "candidates = []\n",
    "postrees = tree\n",
    "\n",
    "from Giveme5W1H.extractor.candidate import Candidate\n",
    "\n",
    "for i, tree in enumerate(postrees):\n",
    "    for candidate in _evaluate_tree(tree):\n",
    "        candidateObject = Candidate()\n",
    "        # used by the extractor\n",
    "        candidateObject.set_raw(candidate[0])  # candidate[0] contains the cause, candidate[1] the effect\n",
    "        candidateObject.set_type(candidate[2])\n",
    "        candidateObject.set_sentence_index(i)\n",
    "\n",
    "        candidates.append(candidateObject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_tree(tree): #추출한 tree를 평가하는 부분!!!\n",
    "        \"\"\"\n",
    "        Determines if the given sub tree contains a cause/effect relation.\n",
    "\n",
    "        The indicators used in this function are inspired by:\n",
    "        \"Automatic Extraction of Cause-Effect Information from Newspaper Text Without Knowledge-based Inferencing\"\n",
    "        by Khoo et. al. (adverbs + conjunctions)\n",
    "        \"Automatic Detection of Causal Relations for Question Answering\" by Roxana Girj (verbs)\n",
    "\n",
    "        :param tree: A tree to analyze\n",
    "        :type tree: ParentedTree\n",
    "\n",
    "        :return: A Tuple containing the cause/effect phrases and the pattern used to find it.\n",
    "        \"\"\"\n",
    "        _candidatesObjects = []\n",
    "        candidates = []\n",
    "        pos = tree.pos() #tree 구조 안에 있는 요소로, 단어의 문장 내 위치를 의미 / read_leaf를 지정해주지 않으면, list형식이어서 pos가 먹히지 않음\n",
    "        tokens = [t[0] for t in pos]\n",
    "\n",
    "        # Searching for cause-effect relations that involve a verb/action we look for NP-VP-NP\n",
    "        for subtree in tree.subtrees(filter=lambda t: t.label() == 'NP' and t.right_sibling() is not None):\n",
    "            sibling = subtree.right_sibling()\n",
    "\n",
    "            # skip to the first verb\n",
    "            while sibling.label() == 'ADVP' and sibling.right_sibling() is not None:\n",
    "                sibling = sibling.right_sibling()\n",
    "\n",
    "            # NP-VP-NP pattern found .__repr__()\n",
    "            if sibling.label() == 'VP' and \"('NP'\" in sibling.__repr__():\n",
    "                verbs = [t[0] for t in sibling.pos() if t[1][0] == 'V'][:3]\n",
    "                verb_synset = set()\n",
    "\n",
    "                # depending on the used tense, we may have to look at the second/third verb e.g. 'have been ...'\n",
    "                for verb in verbs:\n",
    "                    normalized = verb['nlpToken']['originalText'].lower()\n",
    "\n",
    "                    # check if word meaning is relevant\n",
    "                    verb_synset = set(wordnet.synsets(normalized, 'v'))\n",
    "                    if verb_synset.isdisjoint(self.causal_verbs):\n",
    "                        continue\n",
    "\n",
    "                    # if necessary look at the  following phrase\n",
    "                    lemma = self.lemmatizer.lemmatize(normalized)\n",
    "                    if lemma in self.causal_verb_phrases:\n",
    "                        # fetch following two tokens\n",
    "                        rest = ''\n",
    "                        for i, token in enumerate(verbs):\n",
    "                            if verb['nlpToken']['word'] == token['nlpToken']['word']:\n",
    "                                rest = ' '.join([t['nlpToken']['word'] for t in verbs[i + 1:i + 3]]).lower()\n",
    "                                break\n",
    "                        if rest != self.causal_verb_phrases[lemma]:\n",
    "                            continue\n",
    "\n",
    "                # According to Girju, if the found verb is 'cause' or a synonym, the following NP is 100% the cause\n",
    "                # so we can directly put it in the list of candidates\n",
    "                if not verb_synset.isdisjoint(self.constraints_verbs['cause']):\n",
    "                    candidates.append(deepcopy([subtree.pos(), sibling.pos(), 'NP-VP-NP']))\n",
    "                else:\n",
    "                    # pattern contains a valid verb (that is not 'cause'), so check the 7 subpatterns\n",
    "                    pre = [t[0]['nlpToken']['originalText'].lower() for t in subtree.pos() if\n",
    "                           t[1][0] == 'N' and t[0]['nlpToken']['originalText'].isalpha()]\n",
    "                    post = [t[0]['nlpToken']['originalText'].lower() for t in sibling.pos() if\n",
    "                            t[1][0] == 'N' and t[0]['nlpToken']['originalText'].isalpha()]\n",
    "                    pre_con = {'entity': False, 'abstraction': False}\n",
    "                    post_con = {'entity': False, 'phenomenon': False, 'abstraction': False, 'group': False,\n",
    "                                'possession': False, 'event': False, 'act': False, 'state': False}\n",
    "                    verb_con = {'associate': False, 'relate': False, 'lead': False, 'induce': False}\n",
    "\n",
    "                    # check nouns in after verb\n",
    "                    for noun in post:\n",
    "                        noun_synset = set(wordnet.synsets(noun, 'n'))\n",
    "                        for con in post_con:\n",
    "                            post_con[con] = post_con[con] or not noun_synset.isdisjoint(self.constraints_hyponyms[con])\n",
    "                            if post_con['phenomenon']:\n",
    "                                break\n",
    "\n",
    "                        if post_con['phenomenon']:\n",
    "                            break\n",
    "\n",
    "                    # check nouns in before verb\n",
    "                    for noun in pre:\n",
    "                        noun_synset = set(wordnet.synsets(noun, 'n'))\n",
    "                        for con in pre_con:\n",
    "                            pre_con[con] = pre_con[con] or not noun_synset.isdisjoint(self.constraints_hyponyms[con])\n",
    "\n",
    "                    # check if verb is relevant for a subpattern\n",
    "                    for con in verb_con:\n",
    "                        verb_con[con] = not verb_synset.isdisjoint(self.constraints_verbs[con])\n",
    "                        if verb_con[con]:\n",
    "                            break\n",
    "\n",
    "                    # apply subpatterns\n",
    "                    if (\n",
    "                            post_con['phenomenon']\n",
    "                    ) or (\n",
    "                            not pre_con['entity'] and (verb_con['associate'] or verb_con['relate']) and (\n",
    "                            post_con['abstraction'] and post_con['group'] and post_con['possession'])\n",
    "                    ) or (\n",
    "                            not pre_con['entity'] and post_con['event']\n",
    "                    ) or (\n",
    "                            not pre_con['abstraction'] and (post_con['event'] or post_con['act'])\n",
    "                    ) or (\n",
    "                            verb_con['lead'] and (not post_con['entity'] and not post_con['group'])\n",
    "                    ):\n",
    "                        candidates.append(deepcopy([subtree.pos(), sibling.pos(), 'NP-VP-NP']))\n",
    "\n",
    "        # search for adverbs or clausal conjunctions\n",
    "        for i in range(len(tokens)):\n",
    "            token = tokens[i]['nlpToken']['originalText'].lower()\n",
    "\n",
    "            if pos[i][1] == 'RB' and token in self.adverbial_indicators:\n",
    "                # If we come along an adverb (RB) check the adverbials that indicate causation\n",
    "                candidates.append(deepcopy([pos[:i], pos[i - 1:], 'RB']))\n",
    "\n",
    "            elif token in self.causal_conjunctions and ' '.join(\n",
    "                    [x['nlpToken']['originalText'] for x in tokens[i:]]).lower().startswith(\n",
    "                    self.causal_conjunctions[token]):\n",
    "                # Check if token is a clausal conjunction indicating causation\n",
    "                start = i\n",
    "                if token not in self.causal_conjunctions_inclusive:\n",
    "                    # exclude clausal conjunction besides special cases\n",
    "                    start += 1\n",
    "                candidates.append(deepcopy([pos[start:], pos[:i], 'biclausal']))\n",
    "\n",
    "        # drop candidates containing other candidates\n",
    "        unique_candidates = []\n",
    "        candidate_strings = []\n",
    "        for candidate in candidates:\n",
    "            # Bugfix, at some very rare occasions, the candidate holds an empty list\n",
    "            if len(candidate[0]) > 0:\n",
    "                another_string = [x[0]['nlpToken']['originalText'] for x in candidate[1]]\n",
    "                a_string = candidate[0][0][0]['nlpToken']['originalText'] + ' ' + ' '.join(another_string)\n",
    "                candidate_strings.append(a_string)\n",
    "\n",
    "        for i, candidate in enumerate(candidates):\n",
    "            unique = True\n",
    "            for j, substring in enumerate(candidate_strings):\n",
    "                if i != j and candidate[2] == candidates[j][2] and substring in candidate_strings[i]:\n",
    "                    unique = False\n",
    "                    break\n",
    "            if unique:\n",
    "                unique_candidates.append(candidate)\n",
    "\n",
    "        return unique_candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.subtrees(filter=lambda t: t.label() == 'NP' and t.right_sibling() is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}